---
title: "Stock Counts 52 Machine Learning"
author: "Janis Corona"
date: "3/17/2020"
output:
  word_document: default
  html_document: default
---

This script takes the STOCK_COUNTS_52.csv table made in the matrix.Rmd script and runs known algorithms on it for machine learning results using the features in the table of counts from the time specified in making the table.

```{r, error=FALSE, message=FALSE, warning=FALSE}
library(e1071)
library(caret)
library(randomForest)
library(MASS)
library(gbm)

library(dplyr)
library(tidyr)
library(lubridate)
```

These are the lag1 entries to compare to the next day or today stock values where today is the instance date.
```{r}
stocks_65 <- read.csv('ALL_65_stocks_countGroups_1999-01-01_2020-03-15_lag1.csv', sep=',', header=TRUE, 
                     na.strings=c('',' ','NA'))
```


```{r}
head(stocks_65)
```


```{r}
colnames(stocks_65)
```

Create a subset of the 65 stocks data of the AAL stock.
```{r}
sAAL <- subset(stocks_65, stocks_65$stockName =='AAL')
row.names(sAAL) <- sAAL$Date
sAAL <- sAAL[,c(5,9,14:25)]
head(sAAL)
```

```{r}
str(sAAL)
```


Try predicting the number of increasing days this cycle for the AAL stock as a subset.
```{r}
set.seed(12356789)

inTrain <- createDataPartition(y=sAAL$incrDaysThisCycle, p=0.7, list=FALSE)

trainingSet <- sAAL[inTrain,]
testingSet <- sAAL[-inTrain,]

```



```{r}
rfMod <- train(incrDaysThisCycle ~ ., method='rf', data=(trainingSet), 
               trControl=trainControl(method='cv'), number=5)
```

```{r}
plot(rfMod)

```




```{r}
predRF <- predict(rfMod, testingSet)

predDF <- data.frame(predRF, type=testingSet$incrDaysThisCycle)
predDF

sum <- sum(predRF==testingSet$incrDaysThisCycle) 
length <- length(testingSet$incrDaysThisCycle)
accuracy_rfMod <- (sum/length) 
accuracy_rfMod

```


```{r}
results <- c(round(accuracy_rfMod,2), round(100,2))
results <- as.factor(results)
results <- t(data.frame(results))

colnames(results) <- colnames(predDF)
Results <- rbind(predDF, results) 
Results

```


```{r}
knnMod <- train(incrDaysThisCycle ~ .,
                method='knn', preProcess=c('center','scale'),
                tuneLength=10, trControl=trainControl(method='cv'), data=trainingSet)
```


```{r}
plot(knnMod)

```


```{r, error=FALSE, message=FALSE, warning=FALSE}
rpartMod <- train(incrDaysThisCycle ~ ., method='rpart', tuneLength=9, data=trainingSet) 

```

```{r, error=FALSE, message=FALSE, warning=FALSE}
glmMod <- train(incrDaysThisCycle ~ ., 
                method='glm', data=trainingSet) 
```


```{r, error=FALSE, message=FALSE, warning=FALSE}
predKNN <- predict(knnMod, testingSet)
predRPART <- predict(rpartMod, testingSet)
```

```{r, error=FALSE, message=FALSE, warning=FALSE}
predGLM <- predict(glmMod, testingSet)
```


```{r}
length=length(testingSet$incrDaysThisCycle)
```

```{r}
sumKNN <- sum(predKNN==testingSet$incrDaysThisCycle)
sumRPart <- sum(predRPART==testingSet$incrDaysThisCycle)
```

```{r}
sumGLM <- sum(predGLM==testingSet$incrDaysThisCycle)

```

```{r}
accuracy_KNN <- sumKNN/length 
accuracy_RPART <- sumRPart/length 
```

```{r}
accuracy_GLM <- sumGLM/length 

```


```{r}
predDF2 <- data.frame(predRF,predKNN,predRPART,predGLM, 
                      TYPE=testingSet$incrDaysThisCycle)
colnames(predDF2) <- c('RandomForest','KNN','Rpart','GLM','TrueValue')

results <- c(round(accuracy_rfMod,2),  
             round(accuracy_KNN,2), 
             round(accuracy_RPART,2),
             round(accuracy_GLM,2), 
             round(100,2))

results <- as.factor(results)
results <- t(data.frame(results))
colnames(results) <- c('RandomForest','KNN','Rpart','GLM','TrueValue')
Results <- rbind(predDF2, results) 
Results


```


***

When looking at the above table, the errors are based on an exact match, but the values you notice as I do, are rounded up to the true value if greater than zero and zero if less than zero will give very near perfect results for predicting the number of days the cycle will be increasing. Lets test this out.

```{r}
predRF_round <- predict(rfMod, testingSet)

predRF_round <- ifelse(predRF_round<0,0,round(predRF,0))

predDF_round <- data.frame(predRF_round, IncreasingDaysThisCycle=testingSet$incrDaysThisCycle)
predDF_round

sum <- sum(predRF_round==testingSet$incrDaysThisCycle) 
length <- length(testingSet$incrDaysThisCycle)
accuracy_rfMod_round <- (sum/length) 
accuracy_rfMod_round

```


```{r}
results_round <- c(round(accuracy_rfMod_round,2), round(100,2))
results_round <- as.factor(results_round)
results_round <- t(data.frame(results_round))

colnames(results_round) <- colnames(predDF_round)
Results_round <- rbind(predDF_round, results_round) 
Results_round

```


```{r}
knnMod_round <- train(incrDaysThisCycle ~ .,
                method='knn', preProcess=c('center','scale'),
                tuneLength=10, trControl=trainControl(method='cv'), data=trainingSet)
```


```{r, error=FALSE, message=FALSE, warning=FALSE}
rpartMod_round <- train(incrDaysThisCycle ~ ., method='rpart', tuneLength=9, data=trainingSet) 

```

```{r, error=FALSE, message=FALSE, warning=FALSE}
glmMod_round <- train(incrDaysThisCycle ~ ., 
                method='glm', data=trainingSet) 
```


```{r}
predKNN <- predict(knnMod_round, testingSet)
predKNN_round <- ifelse(predKNN<0,0,round(predKNN,0))

predRPART <- predict(rpartMod_round, testingSet)
predRPART_round <- ifelse(predRPART<0,0,round(predRPART,0))
```

```{r, error=FALSE, message=FALSE, warning=FALSE}
predGLM <- predict(glmMod_round, testingSet)
predGLM_round <- ifelse(predGLM<0,0,round(predGLM,0))
```


```{r}
length=length(testingSet$incrDaysThisCycle)
```

```{r}
sumKNN_round <- sum(predKNN_round==testingSet$incrDaysThisCycle)
sumRPart_round <- sum(predRPART_round==testingSet$incrDaysThisCycle)
```

```{r}
sumGLM_round <- sum(predGLM_round==testingSet$incrDaysThisCycle)

```

```{r}
accuracy_KNN_round <- sumKNN_round/length 
accuracy_RPART_round <- sumRPart_round/length 
```

```{r}
accuracy_GLM_round <- sumGLM_round/length 

```


```{r}
predDF2_round <- data.frame(predRF_round,predKNN_round,predRPART_round,predGLM_round, 
                      IncreasingDaysThisCycle=testingSet$incrDaysThisCycle)
colnames(predDF2_round) <- c('RandomForest_round','KNN_round','Rpart_round','GLM_round','IncreasingDaysThisCycle')

results_round <- c(round(accuracy_rfMod_round,2),  
             round(accuracy_KNN_round,2), 
             round(accuracy_RPART_round,2),
             round(accuracy_GLM_round,2), 
             round(100,2))

results_round <- as.factor(results_round)
results_round <- t(data.frame(results_round))
colnames(results_round) <- c('RandomForest_round','KNN_round','Rpart_round','GLM_round','IncreasingDaysThisCycle')
Results_round <- rbind(predDF2_round, results_round) 
Results_round

```

Lets compare this to the table we just saw earlier when the values weren't rounded or zero if less than zero.
```{r}
Results

```

This certainly confirms that these algorithms are great for predicting the number of days that will continue to decrease with the features we selected and the models used in R, random forest, recursive partitioned trees, generalized linear models, and K-nearest neighbors for numeric data. 

We can align the rounded table by indices with our sAAL testing set table to see what dates we can look at for clarity.
```{r}
sAAL$row <- row.names(sAAL)
testingSet$row <- row.names(testingSet)
Results_round$row <- row.names(Results_round)

AAL <- merge(testingSet,Results_round, by.x='row', by.y='row')
AAL
```



***

The above used the cumulative counts with the subsequent step vectors to get the predicted result for number of increasing days in the observation. How well does the machine learning do when removing those subsequent vectors to predict the number of days increasing with each observation? Keep the day of the week but remove the count/group of count columns other than the incrDaysThisCycle and the nTimesIncrDayCountsOccurs columns.
```{r}
sAAL_1 <- sAAL[,c(1:4,13:14)]
colnames(sAAL_1)
```

```{r}
set.seed(12356789)

inTrain <- createDataPartition(y=sAAL_1$incrDaysThisCycle, p=0.7, list=FALSE)

trainingSet <- sAAL_1[inTrain,]
testingSet <- sAAL_1[-inTrain,]

```



```{r}
rfMod <- train(incrDaysThisCycle ~ ., method='rf', data=(trainingSet), 
               trControl=trainControl(method='cv'), number=5)
```

```{r}
plot(rfMod)

```




```{r}
predRF <- predict(rfMod, testingSet)

predDF <- data.frame(predRF, type=testingSet$incrDaysThisCycle)
predDF

sum <- sum(predRF==testingSet$incrDaysThisCycle) 
length <- length(testingSet$incrDaysThisCycle)
accuracy_rfMod <- (sum/length) 
accuracy_rfMod

```


```{r}
results <- c(round(accuracy_rfMod,2), round(100,2))
results <- as.factor(results)
results <- t(data.frame(results))

colnames(results) <- colnames(predDF)
Results <- rbind(predDF, results) 
Results

```


```{r}
knnMod <- train(incrDaysThisCycle ~ .,
                method='knn', preProcess=c('center','scale'),
                tuneLength=10, trControl=trainControl(method='cv'), data=trainingSet)
```


```{r}
plot(knnMod)

```


```{r, error=FALSE, message=FALSE, warning=FALSE}
rpartMod <- train(incrDaysThisCycle ~ ., method='rpart', tuneLength=9, data=trainingSet) 

```

```{r, error=FALSE, message=FALSE, warning=FALSE}
glmMod <- train(incrDaysThisCycle ~ ., 
                method='glm', data=trainingSet) 
```


```{r, error=FALSE, message=FALSE, warning=FALSE}
predKNN <- predict(knnMod, testingSet)
predRPART <- predict(rpartMod, testingSet)
```

```{r, error=FALSE, message=FALSE, warning=FALSE}
predGLM <- predict(glmMod, testingSet)
```


```{r}
length=length(testingSet$incrDaysThisCycle)
```

```{r}
sumKNN <- sum(predKNN==testingSet$incrDaysThisCycle)
sumRPart <- sum(predRPART==testingSet$incrDaysThisCycle)
```

```{r}
sumGLM <- sum(predGLM==testingSet$incrDaysThisCycle)

```

```{r}
accuracy_KNN <- sumKNN/length 
accuracy_RPART <- sumRPart/length 
```

```{r}
accuracy_GLM <- sumGLM/length 

```


```{r}
predDF2 <- data.frame(predRF,predKNN,predRPART,predGLM, 
                      TYPE=testingSet$incrDaysThisCycle)
colnames(predDF2) <- c('RandomForest','KNN','Rpart','GLM','TrueValue')

results <- c(round(accuracy_rfMod,2),  
             round(accuracy_KNN,2), 
             round(accuracy_RPART,2),
             round(accuracy_GLM,2), 
             round(100,2))

results <- as.factor(results)
results <- t(data.frame(results))
colnames(results) <- c('RandomForest','KNN','Rpart','GLM','TrueValue')
Results <- rbind(predDF2, results) 
Results


```


***

When looking at the above table, the errors are based on an exact match, but the values you notice as I do, are rounded up to the true value if greater than zero and zero if less than zero will give very near perfect results for predicting the number of days the cycle will be increasing. Lets test this out.

```{r}
predRF_round <- predict(rfMod, testingSet)

predRF_round <- ifelse(predRF_round<0,0,round(predRF,0))

predDF_round <- data.frame(predRF_round, IncreasingDaysThisCycle=testingSet$incrDaysThisCycle)
predDF_round

sum <- sum(predRF_round==testingSet$incrDaysThisCycle) 
length <- length(testingSet$incrDaysThisCycle)
accuracy_rfMod_round <- (sum/length) 
accuracy_rfMod_round

```


```{r}
results_round <- c(round(accuracy_rfMod_round,2), round(100,2))
results_round <- as.factor(results_round)
results_round <- t(data.frame(results_round))

colnames(results_round) <- colnames(predDF_round)
Results_round <- rbind(predDF_round, results_round) 
Results_round

```


```{r}
knnMod_round <- train(incrDaysThisCycle ~ .,
                method='knn', preProcess=c('center','scale'),
                tuneLength=10, trControl=trainControl(method='cv'), data=trainingSet)
```


```{r, error=FALSE, message=FALSE, warning=FALSE}
rpartMod_round <- train(incrDaysThisCycle ~ ., method='rpart', tuneLength=9, data=trainingSet) 

```

```{r, error=FALSE, message=FALSE, warning=FALSE}
glmMod_round <- train(incrDaysThisCycle ~ ., 
                method='glm', data=trainingSet) 
```


```{r}
predKNN <- predict(knnMod_round, testingSet)
predKNN_round <- ifelse(predKNN<0,0,round(predKNN,0))

predRPART <- predict(rpartMod_round, testingSet)
predRPART_round <- ifelse(predRPART<0,0,round(predRPART,0))
```

```{r, error=FALSE, message=FALSE, warning=FALSE}
predGLM <- predict(glmMod_round, testingSet)
predGLM_round <- ifelse(predGLM<0,0,round(predGLM,0))
```


```{r}
length=length(testingSet$incrDaysThisCycle)
```

```{r}
sumKNN_round <- sum(predKNN_round==testingSet$incrDaysThisCycle)
sumRPart_round <- sum(predRPART_round==testingSet$incrDaysThisCycle)
```

```{r}
sumGLM_round <- sum(predGLM_round==testingSet$incrDaysThisCycle)

```

```{r}
accuracy_KNN_round <- sumKNN_round/length 
accuracy_RPART_round <- sumRPart_round/length 
```

```{r}
accuracy_GLM_round <- sumGLM_round/length 

```


```{r}
predDF2_round <- data.frame(predRF_round,predKNN_round,predRPART_round,predGLM_round, 
                      IncreasingDaysThisCycle=testingSet$incrDaysThisCycle)
colnames(predDF2_round) <- c('RandomForest_round','KNN_round','Rpart_round','GLM_round','IncreasingDaysThisCycle')

results_round <- c(round(accuracy_rfMod_round,2),  
             round(accuracy_KNN_round,2), 
             round(accuracy_RPART_round,2),
             round(accuracy_GLM_round,2), 
             round(100,2))

results_round <- as.factor(results_round)
results_round <- t(data.frame(results_round))
colnames(results_round) <- c('RandomForest_round','KNN_round','Rpart_round','GLM_round','IncreasingDaysThisCycle')
Results_round <- rbind(predDF2_round, results_round) 
Results_round

```

Lets compare this to the table we just saw earlier when the values weren't rounded or zero if less than zero.
```{r}
Results

```

This certainly confirms that these algorithms are great for predicting the number of days that will continue to decrease with the features we selected and the models used in R, random forest, recursive partitioned trees, generalized linear models, and K-nearest neighbors for numeric data. 

We can align the rounded table by indices with our sAAL testing set table to see what dates we can look at for clarity.
```{r}
sAAL_1$Date <- row.names(sAAL_1)
testingSet$Date <- row.names(testingSet)
Results_round$Date <- row.names(Results_round)

AAL_1 <- merge(testingSet,Results_round, by.x='Date', by.y='Date')
tbl <- rbind(head(AAL_1),tail(AAL_1))
tbl
```


```{r}
Results_round$RandomForest_round <-
  as.numeric(paste(Results_round$RandomForest_round))
Results_round$KNN_round <-
  as.numeric(paste(Results_round$KNN_round))
Results_round$Rpart_round <-
  as.numeric(paste(Results_round$Rpart_round))
Results_round$GLM_round <-
  as.numeric(paste(Results_round$GLM_round))
Results_round$IncreasingDaysThisCycle <-
  as.numeric(paste(Results_round$IncreasingDaysThisCycle))



Results_round$mostVoted <- ifelse(
                        (Results_round$RandomForest_round==Results_round$KNN_round |                        Results_round$RandomForest_round==Results_round$Rpart_round |                        Results_round$RandomForest_round==Results_round$GLM_round),
                       
ifelse(Results_round$KNN_round==Results_round$Rpart_round |                           Results_round$KNN_round==Results_round$GLM_round,
        Results_round$RandomForest_round,
        ifelse(Results_round$Rpart_round ==Results_round$GLM_round,
               Results_round$Rpart_round,
               Results_round$RandomForest_round)),
                               ifelse(Results_round$KNN_round==Results_round$Rpart_round |                          Results_round$KNN_round==Results_round$GLM_round,
        Results_round$KNN_round,
        ifelse(Results_round$Rpart_round==Results_round$GLM_round,
               Results_round$Rpart_round,
               ifelse(Results_round$Rpart_round>Results_round$RandomForest_round,
                      Results_round$Rpart_round, 
                      Results_round$RandomForest_round))
      )
)
                          
sMost <- sum(Results_round$mostVoted==Results_round$IncreasingDaysThisCycle)
lMost <- length(Results_round$IncreasingDaysThisCycle)-1
accMost <- sMost/lMost

```


***

Lets run these rounded models on the training set too, to then combine the predicted results of the training and testing models.

```{r}
predRF_round1 <- predict(rfMod, trainingSet)

predRF_round1 <- ifelse(predRF_round1<0,0,round(predRF,0))

predDF_round1 <- data.frame(predRF_round1, IncreasingDaysThisCycle=trainingSet$incrDaysThisCycle)
predDF_round1

sum <- sum(predRF_round1==trainingSet$incrDaysThisCycle) 
length <- length(trainingSet$incrDaysThisCycle)
accuracy_rfMod_round1 <- (sum/length) 
accuracy_rfMod_round1

```


```{r}
results_round1 <- c(round(accuracy_rfMod_round1,2), round(100,2))
results_round1 <- as.factor(results_round1)
results_round1 <- t(data.frame(results_round1))

colnames(results_round1) <- colnames(predDF_round1)
Results_round1 <- rbind(predDF_round1, results_round1) 

Results_round1

```


```{r}
knnMod_round1 <- train(incrDaysThisCycle ~ .,
                method='knn', preProcess=c('center','scale'),
                tuneLength=10, trControl=trainControl(method='cv'), data=trainingSet)
```


```{r, error=FALSE, message=FALSE, warning=FALSE}
rpartMod_round1 <- train(incrDaysThisCycle ~ ., method='rpart', tuneLength=9, data=trainingSet) 

```

```{r, error=FALSE, message=FALSE, warning=FALSE}
glmMod_round1 <- train(incrDaysThisCycle ~ ., 
                method='glm', data=trainingSet) 
```


```{r}
predKNN <- predict(knnMod_round1, trainingSet)
predKNN_round1 <- ifelse(predKNN<0,0,round(predKNN,0))

predRPART <- predict(rpartMod_round1, trainingSet)
predRPART_round1 <- ifelse(predRPART<0,0,round(predRPART,0))
```

```{r, error=FALSE, message=FALSE, warning=FALSE}
predGLM <- predict(glmMod_round1, trainingSet)
predGLM_round1 <- ifelse(predGLM<0,0,round(predGLM,0))
```


```{r}
length=length(trainingSet$incrDaysThisCycle)
```

```{r}
sumKNN_round1 <- sum(predKNN_round1==trainingSet$incrDaysThisCycle)
sumRPart_round1 <- sum(predRPART_round1==trainingSet$incrDaysThisCycle)
```

```{r}
sumGLM_round1 <- sum(predGLM_round1==trainingSet$incrDaysThisCycle)

```

```{r}
accuracy_KNN_round1 <- sumKNN_round1/length 
accuracy_RPART_round1 <- sumRPart_round1/length 
```

```{r}
accuracy_GLM_round1 <- sumGLM_round1/length 

```


```{r}
predDF2_round1 <- data.frame(predRF_round1,predKNN_round1,predRPART_round1,predGLM_round1, 
                      IncreasingDaysThisCycle=trainingSet$incrDaysThisCycle)
colnames(predDF2_round1) <- c('RandomForest_round1','KNN_round1','Rpart_round1','GLM_round1','IncreasingDaysThisCycle')

results_round1 <- c(round(accuracy_rfMod_round1,2),  
             round(accuracy_KNN_round1,2), 
             round(accuracy_RPART_round1,2),
             round(accuracy_GLM_round1,2), 
             round(100,2))

results_round1 <- as.factor(results_round1)
results_round1 <- t(data.frame(results_round1))
colnames(results_round1) <- c('RandomForest_round1','KNN_round1','Rpart_round1','GLM_round1','IncreasingDaysThisCycle')
Results_round1 <- rbind(predDF2_round1, results_round1) 
Results_round1

```



We can align the rounded table by indices with our sAAL training set table to see what dates we can look at for clarity.
```{r}
sAAL_1$Date <- row.names(sAAL_1)
trainingSet$Date <- row.names(trainingSet)
Results_round1$Date <- row.names(Results_round1)

AAL_1 <- merge(trainingSet,Results_round1, by.x='Date', by.y='Date')
tbl <- rbind(head(AAL_1),tail(AAL_1))
tbl
```


```{r}
Results_round1$RandomForest_round1 <-
  as.numeric(paste(Results_round1$RandomForest_round1))
Results_round1$KNN_round1 <-
  as.numeric(paste(Results_round1$KNN_round1))
Results_round1$Rpart_round1 <-
  as.numeric(paste(Results_round1$Rpart_round1))
Results_round1$GLM_round1 <-
  as.numeric(paste(Results_round1$GLM_round1))
Results_round1$IncreasingDaysThisCycle <-
  as.numeric(paste(Results_round1$IncreasingDaysThisCycle))


Results_round1$mostVoted <- ifelse(
                        (Results_round1$RandomForest_round1==Results_round1$KNN_round1 |                        Results_round1$RandomForest_round1==Results_round1$Rpart_round1 |                        Results_round1$RandomForest_round1==Results_round1$GLM_round1),
                       
ifelse(Results_round1$KNN_round1==Results_round1$Rpart_round1 |                           Results_round1$KNN_round1==Results_round1$GLM_round1,
        Results_round1$RandomForest_round1,
        ifelse(Results_round1$Rpart_round1 ==Results_round1$GLM_round1,
               Results_round1$Rpart_round1,
               Results_round1$RandomForest_round1)),
                               ifelse(Results_round1$KNN_round1==Results_round1$Rpart_round1 |                          Results_round1$KNN_round1==Results_round1$GLM_round1,
        Results_round1$KNN_round1,
        ifelse(Results_round1$Rpart_round1==Results_round1$GLM_round1,
               Results_round1$Rpart_round1,
               ifelse(Results_round1$Rpart_round1>
                        Results_round1$RandomForest_round1,
                      Results_round1$Rpart_round1, 
                      Results_round1$RandomForest_round1))
      )
)


```

It turns out that using the training set the model was built on to predict the number of days increasing is worse than using the set aside testing set. 

The model trained on the training set and tested on the training set:
```{r}
tr_Results <- rbind(head(Results_round1),tail(Results_round1))
tr_Results
```
The model trained on the training set and tested on the testing set:
```{r}
te_Results <- rbind(head(Results_round), tail(Results_round))
te_Results
```

It is the same model that takes the best model results by most voted among the four algorithms used on the training and testing sets. When the testing set was used the best model in prediction accuracy for increasing days in that cycle was random forest. And when this model was used on the training set the best model was recursive partitioned trees. 

***

We can test out each of the 65 stocks we have records for. The stocks_65 table has the counts and groups of stock and daily stock values within the time period of that table. To modify the table to return the dates with respective counts and groups of counts within that report, the matrix_automateDateLagStocks2.Rmd file would have to have the date set in the beginning of the scipt, and ran with the number of lags or days in stock value pricing before each instance date within the time interval.It takes 10-20 minutes to produce the table csv ALL_65_stocks_countGroups_YYYY-MM-DD_YYYY_MM_DD_lagN.csv where the first ymd is the beginning of the time series and the 2nd is the date the script ran and lagN is the input number of days to lag behind each daily price ( the value N days earlier).

```{r}
head(stocks_65)
```

```{r}
unique(stocks_65$stockName)
```

Go ahead and select one of the stocks in the list above and analyze where it is in a cycle of decreasing or increasing days as we did with the AAL stock. 

Lets select the YELP stock, it has a limited number of observations that start March 2, 2012 through Mar 13, 2020.
```{r}
sYELP <- subset(stocks_65, stocks_65$stockName=='YELP')
head(sYELP)
```


```{r}
str(sYELP)
```

Lets change the date column to date and the year to a factor.
```{r}
sYELP$Date <- as.Date(sYELP$Date)
sYELP$Year <- as.factor(sYELP$Year)
```


Lets predict the number of decreasing days this cycle based on a select subset of the features displayed above.
```{r}
sYELP1 <- sYELP[,c(4,5,9,14,15,19,20,21,24,25)]
colnames(sYELP1)
```

Lets also add a lead column that will give the next day stock value for YELP using the dplyr package. We can then see how it predicts the next day value.
```{r}
sYELP1$NextDayPrice <- lead(sYELP1$stockDayValue,1)
sYELP1 <- sYELP1[complete.cases(sYELP1$NextDayPrice),]
tail(sYELP1)
```

Since we used the next day price, the stock YELP doesn't have information for the 13th of March 2020 in this data set and the table now extends through 3/12/2020.
Make the row names the date and then remove the date column.
```{r}
row.names(sYELP1) <- as.factor(sYELP1$Date)
sYELP1 <- sYELP1[,-c(1)]
head(sYELP1)
```


Now to predict the number of decreasing days, then to predict the accuracy in next day price of the stock.

```{r}
set.seed(12356789)

inTrain <- createDataPartition(y=sYELP1$decrDaysThisCycle, p=0.7, list=FALSE)

trainingSet <- sYELP1[inTrain,]
testingSet <- sYELP1[-inTrain,]

```



```{r}
rfMod <- train(decrDaysThisCycle ~ ., method='rf', data=(trainingSet), 
               trControl=trainControl(method='cv'), number=5)
```

```{r}
plot(rfMod)

```




```{r}
predRF <- predict(rfMod, testingSet)

predDF <- data.frame(predRF, type=testingSet$decrDaysThisCycle)
predDF

sum <- sum(predRF==testingSet$decrDaysThisCycle) 
length <- length(testingSet$decrDaysThisCycle)
accuracy_rfMod <- (sum/length) 
accuracy_rfMod

```


```{r}
results <- c(round(accuracy_rfMod,2), round(100,2))
results <- as.factor(results)
results <- t(data.frame(results))

colnames(results) <- colnames(predDF)
Results <- rbind(predDF, results) 
Results

```


```{r}
knnMod <- train(decrDaysThisCycle ~ .,
                method='knn', preProcess=c('center','scale'),
                tuneLength=10, trControl=trainControl(method='cv'), data=trainingSet)
```


```{r}
plot(knnMod)

```


```{r, error=FALSE, message=FALSE, warning=FALSE}
rpartMod <- train(decrDaysThisCycle ~ ., method='rpart', tuneLength=9, data=trainingSet) 

```

```{r, error=FALSE, message=FALSE, warning=FALSE}
glmMod <- train(decrDaysThisCycle ~ ., 
                method='glm', data=trainingSet) 
```


```{r, error=FALSE, message=FALSE, warning=FALSE}
predKNN <- predict(knnMod, testingSet)
predRPART <- predict(rpartMod, testingSet)
```

```{r, error=FALSE, message=FALSE, warning=FALSE}
predGLM <- predict(glmMod, testingSet)
```


```{r}
length=length(testingSet$decrDaysThisCycle)
```

```{r}
sumKNN <- sum(predKNN==testingSet$decrDaysThisCycle)
sumRPart <- sum(predRPART==testingSet$decrDaysThisCycle)
```

```{r}
sumGLM <- sum(predGLM==testingSet$decrDaysThisCycle)

```

```{r}
accuracy_KNN <- sumKNN/length 
accuracy_RPART <- sumRPart/length 
```

```{r}
accuracy_GLM <- sumGLM/length 

```


```{r}
predDF2 <- data.frame(predRF,predKNN,predRPART,predGLM, 
                      TYPE=testingSet$decrDaysThisCycle)
colnames(predDF2) <- c('RandomForest','KNN','Rpart','GLM','TrueValue')

results <- c(round(accuracy_rfMod,2),  
             round(accuracy_KNN,2), 
             round(accuracy_RPART,2),
             round(accuracy_GLM,2), 
             round(100,2))

results <- as.factor(results)
results <- t(data.frame(results))
colnames(results) <- c('RandomForest','KNN','Rpart','GLM','TrueValue')
Results <- rbind(predDF2, results) 
Results


```


***

When looking at the above table, the errors are based on an exact match, but the values you notice as I do, are rounded up to the true value if greater than zero and zero if less than zero will give very near perfect results for predicting the number of days the cycle will be increasing. Lets test this out.

```{r}
predRF_round <- predict(rfMod, testingSet)

predRF_round <- ifelse(predRF_round<0,0,round(predRF,0))

predDF_round <- data.frame(predRF_round, IncreasingDaysThisCycle=testingSet$decrDaysThisCycle)
predDF_round

sum <- sum(predRF_round==testingSet$decrDaysThisCycle) 
length <- length(testingSet$decrDaysThisCycle)
accuracy_rfMod_round <- (sum/length) 
accuracy_rfMod_round

```


```{r}
results_round <- c(round(accuracy_rfMod_round,2), round(100,2))
results_round <- as.factor(results_round)
results_round <- t(data.frame(results_round))

colnames(results_round) <- colnames(predDF_round)
Results_round <- rbind(predDF_round, results_round) 
Results_round

```


```{r}
knnMod_round <- train(decrDaysThisCycle ~ .,
                method='knn', preProcess=c('center','scale'),
                tuneLength=10, trControl=trainControl(method='cv'), data=trainingSet)
```


```{r, error=FALSE, message=FALSE, warning=FALSE}
rpartMod_round <- train(decrDaysThisCycle ~ ., method='rpart', tuneLength=9, data=trainingSet) 

```

```{r, error=FALSE, message=FALSE, warning=FALSE}
glmMod_round <- train(decrDaysThisCycle ~ ., 
                method='glm', data=trainingSet) 
```


```{r}
predKNN <- predict(knnMod_round, testingSet)
predKNN_round <- ifelse(predKNN<0,0,round(predKNN,0))

predRPART <- predict(rpartMod_round, testingSet)
predRPART_round <- ifelse(predRPART<0,0,round(predRPART,0))
```

```{r, error=FALSE, message=FALSE, warning=FALSE}
predGLM <- predict(glmMod_round, testingSet)
predGLM_round <- ifelse(predGLM<0,0,round(predGLM,0))
```


```{r}
length=length(testingSet$decrDaysThisCycle)
```

```{r}
sumKNN_round <- sum(predKNN_round==testingSet$decrDaysThisCycle)
sumRPart_round <- sum(predRPART_round==testingSet$decrDaysThisCycle)
```

```{r}
sumGLM_round <- sum(predGLM_round==testingSet$decrDaysThisCycle)

```

```{r}
accuracy_KNN_round <- sumKNN_round/length 
accuracy_RPART_round <- sumRPart_round/length 
```

```{r}
accuracy_GLM_round <- sumGLM_round/length 

```


```{r}
predDF2_round <- data.frame(predRF_round,predKNN_round,predRPART_round,predGLM_round, 
                      DecreasingDaysThisCycle=testingSet$decrDaysThisCycle)
colnames(predDF2_round) <- c('RandomForest_round','KNN_round','Rpart_round','GLM_round','DecreasingDaysThisCycle')

results_round <- c(round(accuracy_rfMod_round,2),  
             round(accuracy_KNN_round,2), 
             round(accuracy_RPART_round,2),
             round(accuracy_GLM_round,2), 
             round(100,2))

results_round <- as.factor(results_round)
results_round <- t(data.frame(results_round))
colnames(results_round) <- c('RandomForest_round','KNN_round','Rpart_round','GLM_round','DecreasingDaysThisCycle')
Results_round <- rbind(predDF2_round, results_round) 
Results_round

```

Lets compare this to the table we just saw earlier when the values weren't rounded or zero if less than zero.
```{r}
Results

```

This certainly confirms that these algorithms are great for predicting the number of days that will continue to decrease with the features we selected and the models used in R, random forest, recursive partitioned trees, generalized linear models, and K-nearest neighbors for numeric data. 

We can align the rounded table by indices with our sAAL testing set table to see what dates we can look at for clarity.
```{r}
sYELP1$Date <- row.names(sYELP1)
testingSet$Date <- row.names(testingSet)
Results_round$Date <- row.names(Results_round)

sYELP1 <- merge(testingSet,Results_round, by.x='Date', by.y='Date')
tbl <- rbind(head(sYELP1),tail(sYELP1))
tbl
```


```{r}
Results_round$RandomForest_round <-
  as.numeric(paste(Results_round$RandomForest_round))
Results_round$KNN_round <-
  as.numeric(paste(Results_round$KNN_round))
Results_round$Rpart_round <-
  as.numeric(paste(Results_round$Rpart_round))
Results_round$GLM_round <-
  as.numeric(paste(Results_round$GLM_round))
Results_round$DecreasingDaysThisCycle <-
  as.numeric(paste(Results_round$DecreasingDaysThisCycle))



Results_round$mostVoted <- ifelse(
                        (Results_round$RandomForest_round==Results_round$KNN_round |                        Results_round$RandomForest_round==Results_round$Rpart_round |                        Results_round$RandomForest_round==Results_round$GLM_round),
                       
ifelse(Results_round$KNN_round==Results_round$Rpart_round |                           Results_round$KNN_round==Results_round$GLM_round,
        Results_round$RandomForest_round,
        ifelse(Results_round$Rpart_round ==Results_round$GLM_round,
               Results_round$Rpart_round,
               Results_round$RandomForest_round)),
                               ifelse(Results_round$KNN_round==Results_round$Rpart_round |                          Results_round$KNN_round==Results_round$GLM_round,
        Results_round$KNN_round,
        ifelse(Results_round$Rpart_round==Results_round$GLM_round,
               Results_round$Rpart_round,
               ifelse(Results_round$Rpart_round>Results_round$RandomForest_round,
                      Results_round$Rpart_round, 
                      Results_round$RandomForest_round))
      )
)
                          
sMost <- sum(Results_round$mostVoted==Results_round$DecreasingDaysThisCycle)
lMost <- length(Results_round$DecreasingDaysThisCycle)-1
accMost <- sMost/lMost
accMost
Results_round
```

The accuracy in prediction is about the same for decreasing days in the cycle as the target instead of the increasing days in the cycle that was the target when predicting the AAL stock increasing days by select features.

Now, lets see how well these algorithms predict the NextDayPrice. But first remove the added predictive measures just done.
```{r}
sYELP1 <- sYELP[,c(4,5,9,14,15,19,20,21,24,25)]
sYELP1$NextDayPrice <- lead(sYELP1$stockDayValue,1)
sYELP1 <- sYELP1[complete.cases(sYELP1$NextDayPrice),]
row.names(sYELP1) <- as.factor(sYELP1$Date)
sYELP1 <- sYELP1[,-c(1)]
```


Now, we can see how well the next day price is predicted.
```{r}
set.seed(12356789)

inTrain <- createDataPartition(y=sYELP1$NextDayPrice, p=0.7, list=FALSE)

trainingSet <- sYELP1[inTrain,]
testingSet <- sYELP1[-inTrain,]

```



```{r}
rfMod <- train(NextDayPrice ~ ., method='rf', data=(trainingSet), 
               trControl=trainControl(method='cv'), number=5)
```

```{r}
plot(rfMod)

```




```{r}
predRF <- predict(rfMod, testingSet)

predDF <- data.frame(predRF, nextDayPrice=testingSet$NextDayPrice)
predDF

sum <- sum(predRF==testingSet$NextDayPrice) 
length <- length(testingSet$NextDayPrice)
accuracy_rfMod <- (sum/length) 
accuracy_rfMod

```


```{r}
results <- c(round(accuracy_rfMod,2), round(100,2))
results <- as.factor(results)
results <- t(data.frame(results))

colnames(results) <- colnames(predDF)
Results <- rbind(predDF, results) 
Results

```


```{r}
knnMod <- train(NextDayPrice ~ .,
                method='knn', preProcess=c('center','scale'),
                tuneLength=10, trControl=trainControl(method='cv'), data=trainingSet)
```


```{r}
plot(knnMod)

```


```{r, error=FALSE, message=FALSE, warning=FALSE}
rpartMod <- train(NextDayPrice ~ ., method='rpart', tuneLength=9, data=trainingSet) 

```

```{r, error=FALSE, message=FALSE, warning=FALSE}
glmMod <- train(NextDayPrice ~ ., 
                method='glm', data=trainingSet) 
```


```{r, error=FALSE, message=FALSE, warning=FALSE}
predKNN <- predict(knnMod, testingSet)
predRPART <- predict(rpartMod, testingSet)
```

```{r, error=FALSE, message=FALSE, warning=FALSE}
predGLM <- predict(glmMod, testingSet)
```


```{r}
length=length(testingSet$NextDayPrice)
```

```{r}
sumKNN <- sum(predKNN==testingSet$NextDayPrice)
sumRPart <- sum(predRPART==testingSet$NextDayPrice)
```

```{r}
sumGLM <- sum(predGLM==testingSet$NextDayPrice)

```

```{r}
accuracy_KNN <- sumKNN/length 
accuracy_RPART <- sumRPart/length 
```

```{r}
accuracy_GLM <- sumGLM/length 

```


```{r}
predDF2 <- data.frame(predRF,predKNN,predRPART,predGLM, 
                      TYPE=testingSet$NextDayPrice)
colnames(predDF2) <- c('RandomForest','KNN','Rpart','GLM','TrueValue')

results <- c(round(accuracy_rfMod,2),  
             round(accuracy_KNN,2), 
             round(accuracy_RPART,2),
             round(accuracy_GLM,2), 
             round(100,2))

results <- as.factor(results)
results <- t(data.frame(results))
colnames(results) <- c('RandomForest','KNN','Rpart','GLM','TrueValue')
Results <- rbind(predDF2, results) 
Results


```


***

When looking at the above table, the errors are based on an exact match, but the values you notice as I do, are rounded up to the true value if greater than zero and zero if less than zero will give very near perfect results for predicting the number of days the cycle will be increasing. Lets test this out. Lets round the NextDayPrice column too, for better accuracy in the testing and training set only.
```{r}
sYELP1 <- sYELP[,c(4,5,9,14,15,19,20,21,24,25)]
sYELP1$NextDayPrice <- lead(sYELP1$stockDayValue,1)
sYELP1 <- sYELP1[complete.cases(sYELP1$NextDayPrice),]
row.names(sYELP1) <- as.factor(sYELP1$Date)
sYELP1 <- sYELP1[,-c(1)]
```


```{r}
set.seed(12356789)

inTrain <- createDataPartition(y=sYELP1$NextDayPrice, p=0.7, list=FALSE)

trainingSet <- sYELP1[inTrain,]
testingSet <- sYELP1[-inTrain,]

trainingSet$NextDayPrice <- round(trainingSet$NextDayPrice,0)
testingSet$NextDayPrice <- round(testingSet$NextDayPrice,0)
```



```{r}
rfMod <- train(NextDayPrice ~ ., method='rf', data=(trainingSet), 
               trControl=trainControl(method='cv'), number=5)
```


```{r}
predRF_round <- predict(rfMod, testingSet)

predRF_round <- ifelse(predRF_round<0,0,round(predRF,0))

predDF_round <- data.frame(predRF_round, NextDayPrice=testingSet$NextDayPrice)
predDF_round

sum <- sum(predRF_round==testingSet$NextDayPrice) 
length <- length(testingSet$NextDayPrice)
accuracy_rfMod_round <- (sum/length) 
accuracy_rfMod_round

```


```{r}
results_round <- c(round(accuracy_rfMod_round,2), round(100,2))
results_round <- as.factor(results_round)
results_round <- t(data.frame(results_round))

colnames(results_round) <- colnames(predDF_round)
Results_round <- rbind(predDF_round, results_round) 
Results_round

```


```{r}
knnMod_round <- train(NextDayPrice ~ .,
                method='knn', preProcess=c('center','scale'),
                tuneLength=10, trControl=trainControl(method='cv'), data=trainingSet)
```


```{r, error=FALSE, message=FALSE, warning=FALSE}
rpartMod_round <- train(NextDayPrice ~ ., method='rpart', tuneLength=9, data=trainingSet) 

```

```{r, error=FALSE, message=FALSE, warning=FALSE}
glmMod_round <- train(NextDayPrice ~ ., 
                method='glm', data=trainingSet) 
```


```{r}
predKNN <- predict(knnMod_round, testingSet)
predKNN_round <- ifelse(predKNN<0,0,round(predKNN,0))

predRPART <- predict(rpartMod_round, testingSet)
predRPART_round <- ifelse(predRPART<0,0,round(predRPART,0))
```

```{r, error=FALSE, message=FALSE, warning=FALSE}
predGLM <- predict(glmMod_round, testingSet)
predGLM_round <- ifelse(predGLM<0,0,round(predGLM,0))
```


```{r}
length=length(testingSet$NextDayPrice)
```

```{r}
sumKNN_round <- sum(predKNN_round==testingSet$NextDayPrice)
sumRPart_round <- sum(predRPART_round==testingSet$NextDayPrice)
```

```{r}
sumGLM_round <- sum(predGLM_round==testingSet$NextDayPrice)

```

```{r}
accuracy_KNN_round <- sumKNN_round/length 
accuracy_RPART_round <- sumRPart_round/length 
```

```{r}
accuracy_GLM_round <- sumGLM_round/length 

```


```{r}
predDF2_round <- data.frame(predRF_round,predKNN_round,predRPART_round,predGLM_round, 
                      NextDayPrice=testingSet$NextDayPrice)
colnames(predDF2_round) <- c('RandomForest_round','KNN_round','Rpart_round','GLM_round','NextDayPrice')

results_round <- c(round(accuracy_rfMod_round,2),  
             round(accuracy_KNN_round,2), 
             round(accuracy_RPART_round,2),
             round(accuracy_GLM_round,2), 
             round(100,2))

results_round <- as.factor(results_round)
results_round <- t(data.frame(results_round))
colnames(results_round) <- c('RandomForest_round','KNN_round','Rpart_round','GLM_round','NextDayPrice')
Results_round <- rbind(predDF2_round, results_round) 
Results_round

```

Lets compare this to the table we just saw earlier when the values weren't rounded or zero if less than zero.
```{r}
Results

```

This certainly confirms that these algorithms are great for predicting the number of days that will continue to decrease with the features we selected and the models used in R, random forest, recursive partitioned trees, generalized linear models, and K-nearest neighbors for numeric data. 

We can align the rounded table by indices with our sAAL testing set table to see what dates we can look at for clarity.
```{r}
sYELP1$Date <- row.names(sYELP1)
testingSet$Date <- row.names(testingSet)
Results_round$Date <- row.names(Results_round)

sYELP1 <- merge(testingSet,Results_round, by.x='Date', by.y='Date')
tbl <- rbind(head(sYELP1),tail(sYELP1))
tbl
```


```{r}
Results_round$RandomForest_round <-
  as.numeric(paste(Results_round$RandomForest_round))
Results_round$KNN_round <-
  as.numeric(paste(Results_round$KNN_round))
Results_round$Rpart_round <-
  as.numeric(paste(Results_round$Rpart_round))
Results_round$GLM_round <-
  as.numeric(paste(Results_round$GLM_round))
Results_round$NextDayPrice <-
  as.numeric(paste(Results_round$NextDayPrice))



Results_round$mostVoted <- ifelse(
                        (Results_round$RandomForest_round==Results_round$KNN_round |                        Results_round$RandomForest_round==Results_round$Rpart_round |                        Results_round$RandomForest_round==Results_round$GLM_round),
                       
ifelse(Results_round$KNN_round==Results_round$Rpart_round |                           Results_round$KNN_round==Results_round$GLM_round,
        Results_round$RandomForest_round,
        ifelse(Results_round$Rpart_round ==Results_round$GLM_round,
               Results_round$Rpart_round,
               Results_round$RandomForest_round)),
                               ifelse(Results_round$KNN_round==Results_round$Rpart_round |                          Results_round$KNN_round==Results_round$GLM_round,
        Results_round$KNN_round,
        ifelse(Results_round$Rpart_round==Results_round$GLM_round,
               Results_round$Rpart_round,
               ifelse(Results_round$Rpart_round>Results_round$RandomForest_round,
                      Results_round$Rpart_round, 
                      Results_round$RandomForest_round))
      )
)
                          
sMost <- sum(Results_round$mostVoted==Results_round$NextDayPrice)
lMost <- length(Results_round$NextDayPrice)-1
accMost <- sMost/lMost
accMost
Results_round
```





***
***
***

Another modified machine could be to add a field that describes the increase or decrease the next day as whether or not the next day price increases by 2% and predict this as yes or no.
```{r}
sYELP1 <- sYELP[,c(4,5,9,14,15,19,20,21,24,25)]
sYELP1$NextDayPrice <- lead(sYELP1$stockDayValue,1)
sYELP1 <- sYELP1[complete.cases(sYELP1$NextDayPrice),]
row.names(sYELP1) <- as.factor(sYELP1$Date)
sYELP1 <- sYELP1[,-c(1)]
sYELP1$NextDay2PercentIncrease <- ifelse(sYELP1$NextDayPrice/sYELP1$stockDayValue >1.02, 1,0)
#sYELP1 <- sYELP1[,-10]
sYELP1$NextDay2PercentIncrease <- as.factor(sYELP1$NextDay2PercentIncrease)
```


```{r}
set.seed(12356789)

inTrain <- createDataPartition(y=sYELP1$NextDay2PercentIncrease, p=0.7, list=FALSE)

trainingSet <- sYELP1[inTrain,]
testingSet <- sYELP1[-inTrain,]

NextDayPrice <- testingSet$NextDayPrice

trainingSet <- trainingSet[,-10]
testingSet <- testingSet[,-10]


```


Some modifications to the data as a factor instead of numeric when copy and 
pasting the code above. The random forest is taking a tree approach to classification of the next day's value as 1 for increased by 2% or a 0 for didn't increase by 2%. Otherwise, the random forest would have done regression as done in previous two runs with the next day price value guess. Below is the caret random forest or you could use the random forest package. Make sure not to keep the round(,0) modification or all values will be NA. If you just run this, then no worries.
```{r}
rfMod <- train(NextDay2PercentIncrease ~ ., method='rf', data=(trainingSet), 
               trControl=trainControl(method='cv'), number=5)


#rfMod <- randomForest(NextDay2PercentIncrease ~ ., data=trainingSet, 
 #                     importance=TRUE,
  #                    proximity=TRUE,)
```


```{r}
predRF <- predict(rfMod, testingSet)

predDF <- data.frame(predRF, NextDay2PercentIncrease=testingSet$NextDay2PercentIncrease)
predDF
```

```{r}

sum <- sum(predRF==testingSet$NextDay2PercentIncrease) 
length <- length(testingSet$NextDay2PercentIncrease)
accuracy_rfMod <- (sum/length) 
accuracy_rfMod

```


```{r}
results <- c(round(accuracy_rfMod,2), round(100,2))
results <- as.factor(results)
results <- t(data.frame(results))

colnames(results) <- colnames(predDF)
Results <- rbind(predDF, results) 
Results

```

From the above, 80% is not bad. You could run this program just using the random forest for if the next day value will increase by 2% and be right 4 out of 5 days. Lets see the rest using the caret package for the classification. The metric was changed to 'Accuracy' or 'Kappa' to use the train() of the caret package to classify instead of regress the features on the target as was done earlier.

```{r}
knnMod <- train(NextDay2PercentIncrease ~ .,
                method='knn', preProcess=c('center','scale'),
                tuneLength=10, metric='Accuracy',
                trControl=trainControl(method='cv'), data=trainingSet)
```


```{r, error=FALSE, message=FALSE, warning=FALSE}
rpartMod <- train(NextDay2PercentIncrease ~ ., 
                        method='rpart', tuneLength=9, metric='Accuracy',
                        data=trainingSet) 

```

```{r, error=FALSE, message=FALSE, warning=FALSE}
glmMod <- train(NextDay2PercentIncrease ~ ., metric='Kappa',
                method='glm', data=trainingSet) 
```


```{r}
predKNN <- predict(knnMod, testingSet)
predKNN
```

```{r}
predRPART <- predict(rpartMod, testingSet)
predRPART
```

```{r, error=FALSE, message=FALSE, warning=FALSE}
predGLM <- predict(glmMod, testingSet)
predGLM
```


```{r}
length=length(testingSet$NextDay2PercentIncrease)
length
```

```{r}
sumKNN <- sum(predKNN==testingSet$NextDay2PercentIncrease)
sumRPart <- sum(predRPART==testingSet$NextDay2PercentIncrease)
sumKNN
sumRPart
```

```{r}
sumGLM <- sum(predGLM==testingSet$NextDay2PercentIncrease)
sumGLM
```

```{r}
accuracy_KNN <- sumKNN/length 
accuracy_RPART <- sumRPart/length 
accuracy_KNN
accuracy_RPART
```

```{r}
accuracy_GLM <- sumGLM/length 
accuracy_GLM
```


```{r}
predDF2 <- data.frame(predRF,predKNN,predRPART,predGLM, 
                      NextDay2PercentIncrease=testingSet$NextDay2PercentIncrease)
colnames(predDF2) <- c('RandomForest','KNN','Rpart','GLM','NextDay2PercentIncrease')

results <- c(round(accuracy_rfMod,2),  
             round(accuracy_KNN,2), 
             round(accuracy_RPART,2),
             round(accuracy_GLM,2), 
             round(100,2))

results <- as.factor(results)
results <- t(data.frame(results))
colnames(results) <- c('RandomForest','KNN','Rpart','GLM',
                       'NextDay2PercentIncrease')
Results <- rbind(predDF2, results) 
Results

```


Lets now align these predicted values to the subset for the stock YELP made earlier.
```{r}
sYELP1$Date <- row.names(sYELP1)
testingSet$Date <- row.names(testingSet)
predDF2$Date <- row.names(testingSet)

sYELP1 <- merge(testingSet,predDF2, by.x='Date', by.y='Date')
tbl <- rbind(head(sYELP1),tail(sYELP1))
tbl
```


```{r}
sYELP2 <- sYELP1

sYELP2$RandomForest <- as.numeric(paste(sYELP2$RandomForest))
sYELP2$Rpart <- as.numeric(paste(sYELP2$Rpart))
sYELP2$GLM <- as.numeric(paste(sYELP2$GLM))
sYELP2$NextDay2PercentIncrease.y <-
  as.numeric(paste(sYELP2$NextDay2PercentIncrease.y))
sYELP2$KNN <- as.numeric(paste(sYELP2$KNN))

sYELP2$mostVoted <- ifelse(rowSums(sYELP2[,12:15])>=2,1,0)
                          
sMost <- sum(sYELP2$mostVoted==sYELP2$NextDay2PercentIncrease.y)
lMost <- length(sYELP2$NextDay2PercentIncrease.y)
accMost <- sMost/lMost
results <- as.data.frame(c(round(accuracy_rfMod,2),  
             round(accuracy_KNN,2), 
             round(accuracy_RPART,2),
             round(accuracy_GLM,2), 
             round(accMost,2),
             round(100,2)))
colnames(results) <- 'Results'
row.names(results) <- c('RandomForest','KNN','Rpart','GLM','mostVoted',
                       'NextDay2PercentIncrease')

results
```

The above accuracy measures for random forest, recursive partitioning trees, k-nearest neighbors, and generalized linear models show that 78-80% of the time the next day can be predicted as either having a two percent increase or not. The following is the table of the first 30 predicted results on the testing set of 605 samples. Also, add back in the NextDayPrice extracted earlier so the results weren't dependent on the next day price when predicting whether the next day would be increased by 2%.
```{r}
sYELP2$NextDayPrice <- NextDayPrice
head(sYELP2,30)
```

```{r}
tail(sYELP2,30)
```

The above is the last 30 of the 605 testing set samples with the next day predictions in predicting whether the value would be 2% more than the previous day.